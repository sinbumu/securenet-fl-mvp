# SecureNet-FL AI 모델 개발 히스토리

이 문서는 연합 학습(Federated Learning) 기반 사기 탐지 AI 모델을 개발하는 과정에서 발생했던 주요 이슈와 해결 과정을 기록합니다.

## 1. 프로젝트 목표

- 여러 은행에 분산된 데이터를 개인정보 유출 없이 안전하게 학습하여, 금융 사기를 탐지하는 고성능 AI 모델을 구축한다.
- **기술 스택**: `Flower` (연합 학습 프레임워크), `XGBoost` (머신러닝 모델)
- **시나리오**: 3개의 은행(클라이언트)이 각자의 데이터를 로컬에서 학습하고, 중앙 서버는 이 학습 결과를 종합하여 글로벌 모델을 개선한다.

## 2. 초기 접근 및 첫 번째 난관

### 초기 설계
- Flower의 표준적인 `FedAvg` (Federated Averaging) 전략을 사용하여 각 클라이언트의 XGBoost 모델 업데이트를 서버에서 평균 내는 방식으로 구현을 시도했습니다.

### 발생 문제: 모델 파라미터 불일치
- 각 클라이언트는 서로 다른 데이터로 학습하므로, XGBoost의 트리 구조가 서로 다르게 생성되었습니다.
- 이로 인해 모델 파라미터의 배열 형태(shape)가 클라이언트마다 달라져, 서버에서 `np.add`를 통해 이를 집계하려 할 때 `ValueError: operands could not be broadcast together with shapes (...)` 오류가 발생하며 학습이 실패했습니다.

## 3. 1차 해결: 집계 전략 변경

### 해결 방안
- `FedAvg`의 '평균' 방식이 트리 기반 모델인 XGBoost에 적합하지 않다고 판단했습니다.
- 서버의 집계 전략을 수정하여, 매 라운드마다 **첫 번째 클라이언트로부터 받은 모델을 다른 모든 클라이언트에게 그대로 전파(broadcast)**하는 '순환(Cyclic)' 방식으로 변경했습니다.
- 이를 통해 모든 클라이언트가 동일한 모델 구조 위에서 순차적으로 학습을 이어가게 하여 파라미터 불일치 문제를 해결하고, 기본적인 연합 학습 플로우를 완성했습니다.

## 4. 두 번째 난관: '껍데기 뿐인' 모델

### 문제 발견: 99.9% 정확도의 함정
- 기본 학습이 완료된 모델의 성능을 평가용 데이터셋으로 검증했습니다.
- **정확도(Accuracy)는 99.87%**로 매우 높게 측정되었지만, **실제 사기(Fraud) 거래는 단 한 건도 탐지하지 못하는 심각한 문제**를 발견했습니다. (재현율 Recall = 0)

### 원인 분석: 극심한 클래스 불균형 (Class Imbalance)
- 전체 거래 데이터 중 사기 거래는 0.13%에 불과했습니다.
- 모델이 어려운 소수 클래스(사기)를 학습하기보다는, **모든 거래를 '정상'이라고 예측하는 쉬운 길을 선택**하여 높은 정확도를 얻는 함정에 빠진 것입니다.

## 5. 최종 해결 및 모델 완성

### 해결 방안: `scale_pos_weight` 파라미터 적용
- XGBoost에서 클래스 불균형 문제를 해결하기 위해 제공하는 `scale_pos_weight` 파라미터를 도입했습니다.
- (정상 거래 수 / 사기 거래 수) 비율인 약 `773`을 가중치로 부여하여, 모델이 사기 거래를 하나 틀렸을 때 받는 페널티를 대폭 상승시켰습니다.
- 이를 통해 모델이 더 이상 소수 클래스를 무시하지 않고 적극적으로 학습하도록 유도했습니다.

### 최종 결과
- `scale_pos_weight`를 적용하여 모델을 재학습하고 다시 평가했습니다.
- **정확도(Accuracy) 99.96%**를 유지하면서, 이전에는 0%였던 **사기 탐지 재현율(Fraud Recall)이 99.8%**로 극적으로 향상되었습니다.
- 최종적으로 **실질적인 사기 탐지 능력을 갖춘 고성능 AI 모델**을 완성할 수 있었습니다.

## 6. 장기적인 개선 방향: Gradient-based Federated Boosting

현재 MVP에서 사용된 '순차적 업데이트' 방식은 `FedAvg`와 XGBoost의 비호환성 문제를 해결하기 위한 효과적인 현실적 대안입니다. 이 방식은 연합학습의 핵심 아이디어(탈중앙화된 학습)를 증명하는 데는 충분하지만, 모든 클라이언트의 학습 결과를 수학적으로 '평균'내는 진정한 의미의 집계 방식은 아닙니다.

장기적인 관점에서 모델의 성능을 극대화하고 개인정보보호를 강화하기 위해서는, XGBoost 연합학습을 위해 설계된 보다 발전된 접근법을 도입해야 합니다.

### 올바른 접근법: 그래디언트 기반 연합 부스팅

이 방식은 클라이언트들이 모델 전체가 아닌, **예측의 잔차(residual) 정보, 즉 그래디언트(gradient)와 헤시안(hessian)만을 서버와 공유**하는 것이 핵심입니다.

#### 작동 원리
1.  **예측 및 오차 계산 (클라이언트)**: 서버로부터 받은 글로벌 모델로 로컬 데이터의 예측을 수행하고, 실제 정답과의 오차(그래디언트, 헤시안)를 계산합니다.
2.  **오차 정보 취합 (서버)**: 클라이언트들은 이 오차 정보만 서버로 전송합니다. 서버는 모든 클라이언트의 오차 정보를 안전하게 취합합니다.
3.  **새로운 트리 생성 (서버)**: 서버는 취합된 오차 정보 전체를 사용하여, 모든 클라이언트의 데이터에 대한 오차를 가장 잘 줄일 수 있는 **최적의 트리(Tree)를 단 하나 생성**합니다.
4.  **모델 업데이트 및 반복**: 새로 생성된 트리를 기존 글로벌 모델에 추가하여 업데이트하고, 이 모델을 다시 클라이언트에게 배포하여 과정을 반복합니다.

이러한 접근법은 모든 데이터를 한곳에 모아 XGBoost를 학습시킨 것과 수학적으로 동일한 결과를 내면서, 원본 데이터는 절대 공유되지 않습니다.

#### 기대 효과
- **수학적 정확성**: 중앙 학습과 동일한 모델을 생성하여 최적의 성능을 보장합니다.
- **강화된 개인정보보호**: 클라이언트는 원본 데이터가 아닌, 파생된 값(그래디언트)만 공유하므로 훨씬 안전합니다.
- **구조적 안정성**: 서버가 항상 단일 트리를 생성하므로, 클라이언트 간 모델 구조가 달라져서 발생했던 `ValueError`를 원천적으로 차단합니다.

### 구체적인 구현 로드맵

이러한 고급 연합학습 기법은 직접 구현하기 복잡하므로, 전문 프레임워크를 도입하는 것을 권장합니다.

1.  **FATE (Federated AI Technology Enabler)**: Webank에서 개발한 오픈소스 연합학습 플랫폼으로, 그래디언트 기반의 연합 XGBoost 모델(SecureBoost)을 강력하게 지원하는 대표적인 솔루션입니다.
2.  **NVIDIA FLARE**: NVIDIA에서 제공하는 유연성이 높은 연합학습 프레임워크로, XGBoost를 포함한 다양한 알고리즘의 연합학습을 지원합니다.
3.  **Flower 커스터마이징 (고급)**: Flower의 `Strategy`를 고도로 커스터마이징하여, 클라이언트가 그래디언트를 계산하여 반환하고 서버는 이를 취합해 새 트리를 훈련시키는 로직을 직접 구현할 수도 있습니다.

따라서 이 프로젝트의 다음 단계는, 현재의 순차적 방식을 **FATE와 같은 전문 프레임워크를 도입하여 '그래디언트 기반 연합 부스팅' 방식으로 전환**하는 것이 될 것입니다. 